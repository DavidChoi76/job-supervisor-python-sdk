{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "üìÉ created session constructor file [job_supervisor_constructor_spark.json]\nüëâ use [Session(\"spark\", useFileConstructor=True)] to create Session interface from constructor file\n\n‚ö†Ô∏è delete password from your code/notebook\nüëâ use useFileConstructor option to create a safe Session interface\n"
    }
   ],
   "source": [
    "from job_supervisor_client import *\n",
    "\n",
    "sparkSession = Session('spark', isJupyter=True, user=\"user\", password=\"password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table>\n<thead>\n<tr><th style=\"text-align:left\">name  </th><th style=\"text-align:left\">ip                        </th><th style=\"text-align:left\">port  </th><th style=\"text-align:left\">isCommunityAccount  </th><th style=\"text-align:left\">useUploadedFile  </th><th style=\"text-align:left\">uploadedFileMustHave                                                             </th></tr>\n</thead>\n<tbody>\n<tr><td style=\"text-align:left\">summa </td><td style=\"text-align:left\">keeling.earth.illinois.edu</td><td style=\"text-align:left\">22    </td><td style=\"text-align:left\">True                </td><td style=\"text-align:left\">True             </td><td style=\"text-align:left\">[&#x27;summa_options.json&#x27;, &#x27;installTestCases_local.sh&#x27;, &#x27;data&#x27;, &#x27;output&#x27;, &#x27;settings&#x27;]</td></tr>\n<tr><td style=\"text-align:left\">spark </td><td style=\"text-align:left\">hadoop01.cigi.illinois.edu</td><td style=\"text-align:left\">50022 </td><td style=\"text-align:left\">False               </td><td style=\"text-align:left\">True             </td><td style=\"text-align:left\">[&#x27;index.py&#x27;]                                                                     </td></tr>\n</tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "sparkSession.destinations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkJob = sparkSession.job() # create new job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'file': '1599778482go62'}"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "sparkJob.upload('./examples/pyspark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "‚úÖ job registered with ID: 1599778497wESc\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<job_supervisor_client.Job.Job at 0x112635cd0>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "sparkJob.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "üìÆJob ID: 1599778497wESc\nüìçDestination: spark\n\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table>\n<thead>\n<tr><th>types          </th><th>message                                                                                                            </th><th>time                    </th></tr>\n</thead>\n<tbody>\n<tr><td>JOB_QUEUED     </td><td>job [1599778497wESc] is queued, waiting for registration                                                           </td><td>2020-09-10T22:54:56.586Z</td></tr>\n<tr><td>JOB_REGISTERED </td><td>job [1599778497wESc] is registered with the supervisor, waiting for initialization                                 </td><td>2020-09-10T22:54:56.618Z</td></tr>\n<tr><td>SSH_CONNECTION </td><td>successfully connected to server [hadoop01.cigi.illinois.edu:50022]                                                </td><td>2020-09-10T22:55:05.263Z</td></tr>\n<tr><td>FILES_UPLOADED </td><td>files uploaded to destination                                                                                      </td><td>2020-09-10T22:55:05.263Z</td></tr>\n<tr><td>JOB_INITIALIZED</td><td>job [1599778497wESc] is initialized, waiting for job completion                                                    </td><td>2020-09-10T22:55:05.263Z</td></tr>\n<tr><td>SCRIPT_ENDED   </td><td>script /home/zimox2/job-supervisor/data/upload/15997784435Gce/1599778482go62/index.py job [1599778497wESc] finished</td><td>2020-09-10T22:55:25.635Z</td></tr>\n<tr><td>JOB_ENDED      </td><td>job [1599778497wESc] is complete                                                                                   </td><td>2020-09-10T22:55:25.964Z</td></tr>\n</tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "sparkJob.events(liveOutput=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "üìÆJob ID: 1599778497wESc\nüìçDestination: spark\n\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table>\n<thead>\n<tr><th style=\"text-align:left\">message                                                                                                                                         </th><th style=\"text-align:left\">time                    </th></tr>\n</thead>\n<tbody>\n<tr><td style=\"text-align:left\">Setting default log level to &quot;                                                                                                                  </td><td style=\"text-align:left\">2020-09-10T22:55:07.624Z</td></tr>\n<tr><td style=\"text-align:left\">WARN&quot;.                                                                                                                                          </td><td style=\"text-align:left\">2020-09-10T22:55:07.624Z</td></tr>\n<tr><td style=\"text-align:left\">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).                                                    </td><td style=\"text-align:left\">2020-09-10T22:55:07.624Z</td></tr>\n<tr><td style=\"text-align:left\">                                                                                                                                                </td><td style=\"text-align:left\">2020-09-10T22:55:07.624Z</td></tr>\n<tr><td style=\"text-align:left\">20/09/10 22:55:09 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</td><td style=\"text-align:left\">2020-09-10T22:55:09.624Z</td></tr>\n<tr><td style=\"text-align:left\">[Stage 0:&gt;                                                          (0 + 2) / 3]                                                                </td><td style=\"text-align:left\">2020-09-10T22:55:25.636Z</td></tr>\n<tr><td style=\"text-align:left\">                                                                                                                                                </td><td style=\"text-align:left\">2020-09-10T22:55:25.636Z</td></tr>\n<tr><td style=\"text-align:left\">Pi is roughly 3.121333\n@flag=[SCRIPT_ENDED:script /home/zimox2/job-supervisor/data/upload/15997784435Gce/1599778482go62/index.py job [1599778497wESc] finished]                                                                                                                                                 </td><td style=\"text-align:left\">2020-09-10T22:55:25.636Z</td></tr>\n</tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "sparkJob.logs(liveOutput=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkJob.download(os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}